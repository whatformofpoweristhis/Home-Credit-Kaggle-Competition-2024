{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MB82eHvUrGBM"},"outputs":[],"source":["import sys\n","from pathlib import Path\n","import os\n","import gc\n","import datetime\n","from glob import glob\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.metrics import roc_auc_score\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","ROOT = '/content/drive/MyDrive'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlFCAa2OrGw6","outputId":"c96bf9d6-c5ec-4689-927b-2b27e75a7c65"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLXJaucPrp4F","outputId":"d6a00e02-a021-4303-e078-02d5b9bb4e37"},"outputs":[],"source":["!pip install polars --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nH8ri2wnrGBN","outputId":"3afe7ff5-ef65-460c-ad69-d630b39b7d7e"},"outputs":[],"source":["start_time_utc = datetime.datetime.now()\n","print(f'Notebook Start Time (UTC): {start_time_utc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVkW3_nVrGBO"},"outputs":[],"source":["class Pipeline:\n","\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","        return df\n","\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n","                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n","        df = df.drop(\"date_decision\", \"MONTH\")\n","        return df\n","\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","                if isnull > 0.98:\n","                    df = df.drop(col)\n","\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","\n","        return df\n","\n","\n","\n","class Aggregator:\n","    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n","    def num_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n","        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]\n","\n","        return expr_max + expr_last + expr_mean\n","\n","    def date_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"D\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n","\n","        return expr_max + expr_last + expr_mean\n","\n","    def str_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        # expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n","        return expr_max + expr_last  # +expr_count\n","\n","    def other_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return expr_max + expr_last\n","\n","    def count_expr(df):\n","        cols = [col for col in df.columns if \"num_group\" in col]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return expr_max + expr_last\n","\n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.other_expr(df) + \\\n","                Aggregator.count_expr(df)\n","\n","        return exprs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r66EKBMxrGBO"},"outputs":[],"source":["def read_file(path, depth=None):\n","    df = pl.read_parquet(path)\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","    if depth in [1,2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","    return df\n","\n","\n","def read_files(regex_path, depth=None):\n","    chunks = []\n","\n","    for path in glob(str(regex_path)):\n","        df = pl.read_parquet(path)\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        if depth in [1, 2]:\n","            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","        chunks.append(df)\n","\n","    df = pl.concat(chunks, how=\"vertical_relaxed\")\n","    df = df.unique(subset=[\"case_id\"])\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5REYG56yrGBO"},"outputs":[],"source":["def feature_eng(df_base, depth_0, depth_1, depth_2):\n","    df_base = (\n","        df_base\n","        .with_columns(\n","            month_decision = pl.col(\"date_decision\").dt.month(),\n","            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n","        )\n","    )\n","    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","    df_base = df_base.pipe(Pipeline.handle_dates)\n","    return df_base\n","\n","\n","def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    return df_data, cat_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PD_YWinprGBO"},"outputs":[],"source":["def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.\n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if str(col_type)==\"category\":\n","            continue\n","\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            continue\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hBg2OXprGBO","outputId":"e5d26a8b-940c-4ecf-edeb-78fcd3539080"},"outputs":[],"source":["%%time\n","\n","ROOT            = Path(ROOT)\n","\n","TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n","TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n","\n","data_store = {\n","    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n","    \"depth_0\": [\n","        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n","        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkAmNEHvrGBO","outputId":"38982c49-83e6-450b-e4fb-c291db6c5ec2"},"outputs":[],"source":["%%time\n","\n","df_train = feature_eng(**data_store)\n","print(\"train data shape:\\t\", df_train.shape)\n","del data_store\n","df_train = df_train.pipe(Pipeline.filter_cols)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRgYGmvCrGBP","outputId":"86f0bb55-b115-4956-d006-5d0939ff1b3c","scrolled":true},"outputs":[],"source":["df_train, cat_cols = to_pandas(df_train)\n","df_train = reduce_mem_usage(df_train)\n","print(\"train data shape:\\t\", df_train.shape)\n","nums=df_train.select_dtypes(exclude='category').columns\n","from itertools import combinations, permutations\n","#df_train=df_train[nums]\n","nans_df = df_train[nums].isna()\n","nans_groups={}\n","for col in nums:\n","    cur_group = nans_df[col].sum()\n","    try:\n","        nans_groups[cur_group].append(col)\n","    except:\n","        nans_groups[cur_group]=[col]\n","del nans_df; x=gc.collect()\n","\n","def reduce_group(grps):\n","    use = []\n","    for g in grps:\n","        mx = 0; vx = g[0]\n","        for gg in g:\n","            n = df_train[gg].nunique()\n","            if n>mx:\n","                mx = n\n","                vx = gg\n","            #print(str(gg)+'-'+str(n),', ',end='')\n","        use.append(vx)\n","        #print()\n","    print('Use these',use)\n","    return use\n","\n","def group_columns_by_correlation(matrix, threshold=0.8):\n","    # 计算列之间的相关性\n","    correlation_matrix = matrix.corr()\n","\n","    # 分组列\n","    groups = []\n","    remaining_cols = list(matrix.columns)\n","    while remaining_cols:\n","        col = remaining_cols.pop(0)\n","        group = [col]\n","        correlated_cols = [col]\n","        for c in remaining_cols:\n","            if correlation_matrix.loc[col, c] >= threshold:\n","                group.append(c)\n","                correlated_cols.append(c)\n","        groups.append(group)\n","        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n","\n","    return groups\n","\n","uses=[]\n","for k,v in nans_groups.items():\n","    if len(v)>1:\n","            Vs = nans_groups[k]\n","            #cross_features=list(combinations(Vs, 2))\n","            #make_corr(Vs)\n","            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n","            use=reduce_group(grps)\n","            uses=uses+use\n","            #make_corr(use)\n","    else:\n","        uses=uses+v\n","    print('####### NAN count =',k)\n","print(uses)\n","print(len(uses))\n","uses=uses+list(df_train.select_dtypes(include='category').columns)\n","print(len(uses))\n","df_train=df_train[uses]\n","# df_train.drop(['requesttype_4525192L_cnt','max_empl_employedtotal_800L_cnt', 'max_empl_industry_691L_cnt'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-3gazYqrGBP"},"outputs":[],"source":["y = df_train[\"target\"]\n","weeks = df_train[\"WEEK_NUM\"]\n","df_train= df_train.drop(columns=[\"target\",\"case_id\", \"WEEK_NUM\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8jvHrg1rGBP"},"outputs":[],"source":["df_train[cat_cols] = df_train[cat_cols].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKZBa-HZr69i","outputId":"02723ccf-1aba-459d-d1ed-7496550023b5"},"outputs":[],"source":["!pip install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHfYL7oNrGBP","outputId":"19e3fff4-da64-4300-9346-df543da77b7b","scrolled":true},"outputs":[],"source":["from catboost import CatBoostClassifier, Pool\n","\n","params = {\n","    \"eval_metric\": \"AUC\",  # 相当于LightGBM中的metric参数\n","    # \"depth\": 10,  # 相当于LightGBM中的max_depth参数\n","    \"learning_rate\": 0.03,\n","    \"iterations\":15000,  # 6000\n","    # \"random_seed\": 3107,  # 相当于LightGBM中的random_state参数\n","    # \"l2_leaf_reg\": 20,  # 相当于LightGBM中的reg_lambda参数\n","    # \"border_count\": 254,  # 没有直接相当于LightGBM中的colsample_by*参数，但可以用来增加分箱数\n","    # \"auto_class_weights\": \"Balanced\",\n","    \"verbose\": False,  # 控制输出信息\n","    \"task_type\": \"GPU\",  # 使用GPU训练\n","    \"od_type\": \"Iter\",\n","    \"od_wait\": 100\n","}\n","\n","n_splits = 5\n","fitted_models = []\n","cv_scores = []\n","\n","\n","cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=False)\n","\n","step = 0\n","for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#   Because it takes a long time to divide the data set,\n","    step += 1\n","    print(f'current step: {step}')\n","\n","    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# each time the data set is divided, two models are trained to each other twice, which saves time.\n","    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n","    # case_id=X_valid['case_id']\n","    # X_valid=X_valid.drop(columns=['case_id'])\n","    # X_train=X_train.drop(columns=['case_id'])\n","    train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n","    val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n","\n","    model = CatBoostClassifier(**params)\n","    model.fit(train_pool, eval_set=val_pool, verbose=1000)\n","\n","\n","    fitted_models.append(model)\n","    y_pred_valid = model.predict_proba(X_valid)[:,1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores.append(auc_score)\n","\n","print(\"CV AUC scores: \", cv_scores)\n","print(\"AVG CV AUC score: \", np.mean(cv_scores))\n","print(\"Maximum CV AUC score: \", max(cv_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Sy9rkPUrGBP","outputId":"af71da3f-f432-4a4d-fa55-008cefb893ea"},"outputs":[],"source":["import joblib\n","\n","joblib.dump(fitted_models, 'cat_models.joblib')\n","\n","notebook_info = {\n","    'notebook_start_time': start_time_utc,\n","    'description': 'first cat models',\n","    'cols': df_train.columns.to_list(),\n","    'cat_cols': cat_cols,\n","}\n","joblib.dump(notebook_info, 'notebook_info.joblib')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LK-_HOSgrGBP","outputId":"89c223d9-94a3-44a9-a3b7-587dcaa78c27"},"outputs":[],"source":["!ls -al"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2159.116896,"end_time":"2024-04-07T10:19:38.712675","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T09:43:39.595779","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
